{"title":"Visualizing CNNs","markdown":{"yaml":{"title":"Visualizing CNNs","author":"Aleksandra Muciek","date":"2023-03-27","categories":["ComputerVision","code","python"],"toc":true,"format":{"html":{"code-fold":true}}},"headingText":"Convolutional neural nets","containsRefs":false,"markdown":"\n\n\n\nEven though the concept of convolutional neural networks is not new to me, I noticed a gap in my understanding. I couldn't figure out how to approach visualization of convolutional neural network layers' weights. It alweays seemed odd to me when I looked at examples found online. \n\nHere I will try to explore the concept to teach myself this interesting thing.\n\n\nIn convolutional neural networks we operate on images. Each one is 2D grid of values representing every pixel. If the picture is in grayscale we only care for how dark or light each pixel is, so each image is represented as grid $n_{row} \\times n_{col}$ with values representing darkness of each pixel.\n\nIf we use color pictures, we divide it into $3$ channels - red, green, blue. For image with pixel size $n_{row} \\times n_{col}$ we use one grid for red coulour, second for green and third for blue. This means each picture is represented with 3 grids one on top of the other. Dimensionality of this thing is $(n_{row}, n_{col}, 3)$.\n\n![Layers of images. On the left grayscale image represented by only one matrix, on the right 3 channels of a colorful image.](imgaes_layers.png)\n\n### Filters\n\nFilters are introduced to smartly downscale an image while distilling the most important information convayed by the image. If we go back to grayscale setting, and assume we have $(n_{row}, n_{col}, 1)$ image, a single grid with pixel values, we introduce filter, for example $2 \\times 2$ matrix that will be put on top of the image, place by place where it fits. Then we will calculate elementwise product of each overlaping entries, sum the values and we will end up with a smaller image.\n\n![Example of grayscale image and $2\\times 2$ filter](grayscale_image_and_filter.png)\n\n\n![Applying $2 \\times 2$ filter to the example above.](applying_filters.png)\n\nWhile constructing a concloutional layer in the model, we specify the size of a filter ($2 \\times 2$, $3 \\times 3$ etc.) and we decide on the number of filters we want to apply. In the example above we used only one filter but we could repeat the above process with the second one with different values than $(1, 0),(2, 4)$. In the example above we got output  with dimensions $(3, 3, 1, 1)$. If we chose to apply second filter we would get second result, so overal thing would have dimensions $(3, 3, 1, 2)$. For specified number of filters $k$ we would get $(3, 3, 1, k)$ dimensional tensor. \n\nValues of the filters are not specified by us before training - they are learned by the model. These are the things we may want ti visualize later to see how the model behaves.\n\nIf we take colorful image as input, with size $(n_{row}, n_{col}, 3)$, we construct filters that also have $3$ channels - for example with size $(2, 2, 3)$. Each of the channels of the filter will be applied to the corresponding channel of a picture, the elementwise product will be calculated for each channel separately, as above. Then, before putting it all to the result matrix, the values will be summed together. We then would get flat $(n_{row}', n_{col}', 1)$ output. As before, we are not only applying one filter on the image - we can to this with many filters ($k$), so all of them will be stored as, for example $(2, 2, 3, k)$. And we will get output for each filter, so overal size will be  $(n_{row}', n_{col}', k)$\n\n## Get convolutional model\n\nWe get a trained model from `keras` package called VGG16. Out of this model we will look for convolutional layers.\n\n```{python}\nimport numpy as np\n\n\nfrom keras.applications.vgg16 import VGG16\nmodel = VGG16()\n\n#model.summary()\n\nfor idx, layer in enumerate(model.layers):\n    if 'conv' in layer.name:\n        filters, biases = layer.get_weights()\n        print(f\"Index: {idx})\", layer.name, filters.shape)\n    else:\n        print(f\"Index: {idx})\", layer.name)\n\n#weights from the second layer\n\nfilters, biases = model.layers[1].get_weights()\n\n#scaling\nf_min, f_max = filters.min(), filters.max()\nfilters = (filters - f_min) / (f_max - f_min)\n```\n\nWith the explanation of the layers we can look at the layers in this model. First we get an input. Then we have first convolutional layer. From $(3, 3, 3, 64)$ we know that we have here $3 \\times 3$ filters with $3$ channels (input must be colorful) and the number of these filters is $64$. After applying these filters on the image $(n_{row}, n_{col}, 3)$ we will get \"image\" with shape $(n_{row}', n_{col}', 64)$. \n\nIn the next layer we will have $3 \\times 3$ filters that will need to deal wtth $64$-layer images. We choose to produce $64$ such filters and so on.\n\nOther layers in this model that appear often are pooling layers. They just take image as it is and create smaller versions of it by taking for each small part of an image the maximum value (*MaxPooling*) or average value of the piece (*AvgPooling*). They don't change depth or number of dimensions. Only 2D shape of an image $(n_{row}', n_{col}') \\to (n_{row}'', n_{col}'')$.\n\n\nAt the end of the model we are flattening everything to vectors and create fully connected neural nets to produce predictions at the end.\n\n## Plot some filters\n\nWe can take the first convolutional layer and take a look at the filters - we have there $64$ filters, each has 3 channels and each of these channels is just $3 \\times 3$ grid. We can create \"heatmap\" of this.\n\n```{python}\n#| label: fig-bnwfilters\n#| fig-cap: \"Every row is a visualized 3 x 3 filter with every column being a channel of that filter. \"\nimport matplotlib.pyplot as plt\n\nnum_filters = 4\n\nfig, axs = plt.subplots(num_filters, filters.shape[2], sharey=True, figsize=(8, 12))\nfig.suptitle(\"Visualization of weights\")\nfor i in range(num_filters):\n    filter_0 = filters[:,:,:, i]\n    for j in range(filter_0.shape[2]):\n        axs[i,j].imshow(filter_0[:,:, j], cmap='gray')\n```\n\nSince in this layer every filter has three channels (each corresponding to a channel of a picture), we can in this case plot them as RGB figures.\n\n```{python}\n#| label: fig-rgbfilters\n#| fig-cap: \"Filters with all 3 channels displayed as RGB pictures.\"\nfig, axs = plt.subplots(5, 5, sharey=True, figsize = (8,8))\nfig.suptitle(\"Filters with all 3 channels\")\nfor i in range(5):\n    for j in range(5):\n        axs[i, j].imshow(filters[:,:,:, i*5 + j])\n\n```\n\nThis may look nice but I don't think it is very informative. We can use the filters to transform a real picture and see the results.\n\n## Plot picture after applying filters\n\nWe can take a royalty free picture of a bird and put it through the first convolutional layer of the model.\n\n![Royalty free bird](bird.jpg)\n\n```{python}\n#| output: false\nfrom keras.applications.vgg16 import preprocess_input\nfrom tensorflow.keras.utils import load_img #keras.preprocessing.image is depricated\nfrom tensorflow.keras.utils import img_to_array\nfrom keras.models import Model\n\nmodel_one_layer = Model(inputs=model.inputs, outputs=model.layers[1].output)\nimg = load_img('bird.jpg', target_size=(224, 224))\n\n#image to array\nimg = img_to_array(img)\nimg = np.expand_dims(img, axis=0)\n#(num_samples, n_rows, n_cols, channels)\n\nimg = preprocess_input(img)\n\nfeature_maps = model_one_layer.predict(img)\n```\n\n```{python}\n\nsquare = 5\n\nfig, axs = plt.subplots(square, square, sharey=True, figsize = (8,8))\nfig.suptitle(\"Image treated with filters\")\n\nfor i in range(square):\n    for j in range(square):\n        axs[i, j].imshow(feature_maps[0,:,:, i*square + j], cmap = 'gray')\n\n```\n\n\n## Useful links\n\n- Picture of the bird: [link](https://unsplash.com/photos/3lGi0BXJ1W0?utm_source=unsplash&utm_medium=referral&utm_content=creditShareLink)\n- Machine Learning Mastery blog post: [link](https://machinelearningmastery.com/how-to-visualize-filters-and-feature-maps-in-convolutional-neural-networks/)\n- Towards Data Science blog post on convolutional layers: [link](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)\n- Paper from 2013 on understanding convolutional layers: [link](https://arxiv.org/pdf/1311.2901.pdf)\n- Paper from 2015 on deep visualization: [link](https://yosinski.com/media/papers/Yosinski__2015__ICML_DL__Understanding_Neural_Networks_Through_Deep_Visualization__.pdf)\n- Distilled paper from 2020 on visualizing weghts: [link](https://distill.pub/2020/circuits/visualizing-weights/)\n\n\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"weights_cv.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.475","theme":"flatly","title-block-banner":true,"title":"Visualizing CNNs","author":"Aleksandra Muciek","date":"2023-03-27","categories":["ComputerVision","code","python"]},"extensions":{"book":{"multiFile":true}}}}}